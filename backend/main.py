import json, os
from pathlib import Path
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
from openai import OpenAI
from fastapi.responses import PlainTextResponse
import re

# ------------------------------
# â‘  .env ë¶ˆëŸ¬ì˜¤ê¸°
# ------------------------------
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


# Desert Logic Engine
class DesertLogicEngine:
    def __init__(self, case_id: str):
        rules_path = os.path.join(os.path.dirname(__file__), "logic_engine", "rules", f"{case_id}_rules.json")
        if os.path.exists(rules_path):
            with open(rules_path, "r", encoding="utf-8") as f:
                self.rules = json.load(f)
        else:
            self.rules = []

    def evaluate_text(self, text: str):
        """í…ìŠ¤íŠ¸ ë‚´ ë…¼ë¦¬ ê·œì¹™ ë§¤ì¹­ (yes/no/irrelevant ë°˜í™˜)"""
        for rule in self.rules:
            if re.search(rule["pattern"], text, re.IGNORECASE):
                return {"verdict": rule["verdict"], "evidence": rule.get("evidence", "")}
        return {"verdict": "unknown", "evidence": ""}

    def evaluate_dialogue(self, user_input: str, ai_reply: str):
        user_eval = self.evaluate_text(user_input)
        if user_eval["verdict"] == "yes":
            return f"ë…¼ë¦¬ ì¼ì¹˜: {user_eval['evidence']}"
        return None
        """GPT ì‘ë‹µì„ ì‚¬ê±´ ë…¼ë¦¬ì™€ ëŒ€ì¡°
        ai_eval = self.evaluate_text(ai_reply)
        user_eval = self.evaluate_text(user_input)

        if ai_eval["verdict"] == "no":
            return f"ğŸ¤” ë…¼ë¦¬ ë¶ˆì¼ì¹˜: {ai_eval['evidence']}"
        elif ai_eval["verdict"] == "yes":
            return f"ğŸ§© ë…¼ë¦¬ ì¼ì¹˜: {ai_eval['evidence']}"
        elif user_eval["verdict"] == "yes":
            return f"ğŸ’¬ í¥ë¯¸ë¡œìš´ ë‹¨ì„œì˜ˆìš”. ({user_eval['evidence']})"
        else:
            return None
        """
    # ë‹¨ì„œ ì¶”ì¶œ ë©”ì„œë“œ
    def extract_clue_from_feedback(self, feedback_text: str):
        """í”¼ë“œë°± ë¬¸ì¥ì—ì„œ ë‹¨ì„œëª…ë§Œ ì¶”ì¶œ"""
        match = re.search(r"['\"](.+?)['\"]", feedback_text)
        if match:
            return match.group(1)
        return None

app = FastAPI()
# ------------------------------
# FastAPI ê¸°ë³¸ ì„¤ì •
# ------------------------------
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ì„ì‹œë¡œ ëª¨ë“  origin í—ˆìš© (í…ŒìŠ¤íŠ¸ìš©)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

BASE_DIR = Path(__file__).resolve().parent

# =============================
# ì‚¬ê±´ ë°ì´í„° ë¡œë”
# =============================
def load_case_data(mode: str):
    base_dir = os.path.join(os.path.dirname(__file__), "cases")
    filename = {
        "ìƒ": "case_high.json",
        "ì¤‘": "case_mid.json",
        "í•˜": "case_low.json",
    }.get(mode, "case_low.json")
    path = os.path.join(base_dir, filename)
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


# =============================
# GPT ì‘ë‹µ ìƒì„± (íˆìŠ¤í† ë¦¬ ë°˜ì˜)
# =============================
def generate_gpt_response(case_data, user_input, history):
    """
    ì‚¬ê±´ ë°ì´í„°ì™€ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ GPT ì‘ë‹µì„ ìƒì„±.
    chatbot_instructions(role, style, guidelines)ì„ í†µí•©í•˜ì—¬ AI í˜ë¥´ì†Œë‚˜ë¥¼ êµ¬ì„±.
    """

    chatbot_instr = case_data.get("chatbot_instructions", {})
    role = chatbot_instr.get("role", "ë„ˆëŠ” ì‚¬ê±´ì„ ë¶„ì„í•˜ëŠ” ë¦¬í¬í„° AIì•¼.")
    style = chatbot_instr.get("style", "ëƒ‰ì •í•˜ê³  ë…¼ë¦¬ì ì¸ ë§íˆ¬ë¡œ ë‹µí•´.")
    guidelines_list = chatbot_instr.get("guidelines", [])

    # guidelines ë°°ì—´ì„ ë³´ê¸° ì¢‹ê²Œ ì¤„ ë‹¨ìœ„ë¡œ í•©ì¹¨
    if guidelines_list:
        guidelines = "\n".join(f"- {g}" for g in guidelines_list)
    else:
        guidelines = "ëŒ€í™”ì˜ ë§¥ë½ê³¼ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ë©°, ë…¼ë¦¬ì ì¸ ì¶”ë¡ ì„ ì´ì–´ê°€ì„¸ìš”."

    # System Prompt (AI ìºë¦­í„°ì˜ ì„¸ê³„ê´€, ë§íˆ¬, ê·œì¹™ì´ ëª¨ë‘ í¬í•¨ë¨)
    system_prompt = f"""
{role}
{style}

ë‹¤ìŒì€ ë„¤ ëŒ€í™” ì§€ì¹¨ì´ì•¼:
{guidelines}
    """.strip()

    # ì‚¬ê±´ ê°œìš” ì¶”ê°€ (AIì—ê²Œ ì‚¬ê±´ ì „ì²´ ë§¥ë½ì„ ì œê³µ)
    overview_text = case_data.get("case_overview", "ì‚¬ê±´ ê°œìš”ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [
        {"role": "system", "content": f"{system_prompt}\n\nì‚¬ê±´ ê°œìš”:\n{overview_text}"}
    ]

    # ì´ì „ ëŒ€í™” ë°˜ì˜
    for h in history:
        messages.append({"role": h["role"], "content": h["text"]})

    # ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
    messages.append({"role": "user", "content": user_input})

    # GPT í˜¸ì¶œ (client ì‚¬ìš©)
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages
    )

    # ìµœì¢… ì‘ë‹µ ë°˜í™˜
    return completion.choices[0].message.content.strip()


# =============================
# API: ëŒ€í™” ì—”ë“œí¬ì¸íŠ¸
# =============================
@app.post("/api/chat")
async def chat_endpoint(request: Request):
    data = await request.json()
    user_input = data.get("message", "")
    mode = data.get("mode", "í•˜")
    history = data.get("history", [])

    case_data = load_case_data(mode)
    case_id = f"case_{'high' if mode == 'ìƒ' else 'mid' if mode == 'ì¤‘' else 'low'}"
    logic_engine = DesertLogicEngine(case_id)

    # ì¸ë¬¼ ì§ì ‘ ì§ˆë¬¸ ì‹œ ì¦‰ì‹œ ë°˜í™˜
    for c in case_data["characters"]:
        if c["name"] in user_input:
            return {
                "reply": f"{c['name']} â€” {c['description']}",
                "clue": c["name"],  # clue í•„ë“œ ì¶”ê°€
            }

    # ì¦ê±° ì§ì ‘ ì§ˆë¬¸ ì‹œ ì¦‰ì‹œ ë°˜í™˜
    for e in case_data["evidence"]:
        name = e.get("name")
        if name and name in user_input:
            return {
                "reply": f"{name} ê´€ë ¨ ë‹¨ì„œ: {e.get('description', '')}",
                "clue": name,  # clue í•„ë“œ ì¶”ê°€
            }


    # GPT ì‘ë‹µ ìƒì„± (íˆìŠ¤í† ë¦¬ ë°˜ì˜)
    ai_reply = generate_gpt_response(case_data, user_input, history)

    # Desert Logic ê²€ì¦(Desert Logicì´ ê°ì§€í•œ ë‹¨ì„œ/ë…¼ë¦¬ í‰ê°€ ê²°ê³¼)
    logic_feedback = logic_engine.evaluate_dialogue(user_input, ai_reply)

    # ê°ì§€ëœ ë‹¨ì„œ ì¶”ì¶œ
    clue_data = None
    if logic_feedback:
        # ë¬¸ìì—´ ì•ˆì—ì„œ "ë‹¨ì„œ:" ë’¤ì— ìˆëŠ” ë‹¨ì–´ë§Œ ì¶”ì¶œ (í˜¹ì€ ì§ì ‘ ì „ë‹¬)
        clue_data = logic_engine.extract_clue_from_feedback(logic_feedback)


    # ìµœì¢… ì‘ë‹µ
    if logic_feedback:
        final_reply = f"{ai_reply}\n\n---\nğŸ§  {logic_feedback}"
    else:
        final_reply = ai_reply

    return {
        "reply": final_reply,  # í”„ë¡ íŠ¸ì—”ë“œë¡œ ì „ì†¡ë˜ëŠ” JSON ì‘ë‹µ
        "clue": clue_data  # ê°ì§€ëœ ë‹¨ì„œ í¬í•¨
        } 


# =============================
# ì‚¬ê±´ ì •ë³´ ë°˜í™˜, ìŠ¤í† ë¦¬ txt ë°˜í™˜
# =============================
@app.get("/api/report")
async def report_endpoint(mode: str = "í•˜"):
    case_data = load_case_data(mode)
    
    # ìŠ¤í† ë¦¬ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    story_path = os.path.join(os.path.dirname(__file__), "cases", "story")
    story_file = {
        "ìƒ": "story_high.txt",
        "ì¤‘": "story_mid.txt",
        "í•˜": "story_low.txt",
    }.get(mode, "story_low.txt")
    story_fullpath = os.path.join(story_path, story_file)

    # ìŠ¤í† ë¦¬ í…ìŠ¤íŠ¸ ë¡œë“œ
    story_text = ""
    if os.path.exists(story_fullpath):
        with open(story_fullpath, "r", encoding="utf-8") as f:
            story_text = f.read()
    else:
        story_text = "ìŠ¤í† ë¦¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ì‚¬ê±´ ë°ì´í„° + ìŠ¤í† ë¦¬ í•¨ê»˜ ë°˜í™˜
    return {"case": case_data, "story": story_text}

# =============================
# AI ì±„ì  API
# =============================
@app.post("/api/submit_answer")
async def submit_answer(request: Request):
    data = await request.json()
    mode = data.get("mode", "í•˜")
    user_answer = data.get("answer", "")

    case_data = load_case_data(mode)
    solution = case_data.get("solution", {})
    reference = json.dumps(solution, ensure_ascii=False)

    prompt = f"""
    ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì¶”ë¦¬ ë‹µë³€ì…ë‹ˆë‹¤.
    ì‚¬ê±´ ì œëª©: {case_data.get('title')}
    ì •ë‹µ ë°ì´í„°: {reference}
    ì‚¬ìš©ì ë‹µë³€: {user_answer}
    
    âš ï¸ ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì—ëŠ” 0ì ì„ ì£¼ê³  ê°„ë‹¨íˆ í”¼ë“œë°±í•˜ì„¸ìš”:
    - ì‚¬ìš©ìê°€ ë‹¨ìˆœíˆ "ì¶”ë¦¬ ì‘ì„±", "ì‚¬ê±´ ê°œìš”", "ì—”ë”© ë³´ê¸°" ë“± í˜•ì‹ì  ë¬¸êµ¬ë§Œ ì…ë ¥í•œ ê²½ìš°
    - ì¶”ë¦¬ì˜ ë‚´ìš©ì´ ì „í˜€ ì—†ëŠ” ê²½ìš° (ë²”ì¸, ë™ê¸°, ì‚¬ê±´ ë‚´ìš© ì—†ìŒ)
    - ë‹¨ìˆœí•œ ëª…ë ¹ë¬¸, í…ŒìŠ¤íŠ¸ ë¬¸ì¥, í•œ ì¤„ì§œë¦¬ ì…ë ¥

    ê¸°ì¤€:
    1. ë²”ì¸, ë™ê¸°, ë°©ë²•, ê²°ë¡  ì¼ì¹˜ë„ (ì´ì  100ì )
    2. ë…¼ë¦¬ ì¼ê´€ì„± ë° ë‹¨ì„œ í™œìš©ë„ (+/- 20ì  ê°€ì¤‘)
    3. í•µì‹¬ ì§„ì‹¤ ëˆ„ë½ ì‹œ ê°ì 
    4. í”¼ë“œë°±ì€ ê°„ê²°í•˜ê³  ì¸ë¬¼Â·ì¦ê±° ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±

    JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
    {{
      "score": <0~100>,
      "feedback": "<ì§§ì€ í‰ê°€ ì½”ë©˜íŠ¸>"
    }}
    """

    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ê³µí¬ ì¶”ë¦¬ ê²Œì„ì˜ ì±„ì ê´€ AIì•¼. ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ë˜, ì•½ê°„ ë¶ˆì•ˆí•œ ë§íˆ¬ë¥¼ ì¨."},
            {"role": "user", "content": prompt}
        ]
    )

    try:
        result = json.loads(completion.choices[0].message.content)
    except:
        result = {"score": 0, "feedback": "ì±„ì  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

    return result