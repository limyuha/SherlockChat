import json, os, re
from pathlib import Path
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
from openai import OpenAI
from fastapi.responses import PlainTextResponse
from functools import lru_cache

import time, traceback


# ------------------------------
# â‘  .env ë¶ˆëŸ¬ì˜¤ê¸°
# ------------------------------
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

with open("logic_engine/rules/case_mid_rules.json", "r", encoding="utf-8") as f:
    rules = json.load(f)

for r in rules:
    pattern = r.get("pattern", "")
    if not pattern.strip():
        print("âš ï¸ ë¹ˆ íŒ¨í„´ ê°ì§€:", r)



# Desert Logic Engine
class DesertLogicEngine:
    def __init__(self, case_id: str):
        rules_path = os.path.join(os.path.dirname(__file__), "logic_engine", "rules", f"{case_id}_rules.json")
        if os.path.exists(rules_path):
            with open(rules_path, "r", encoding="utf-8") as f:
                self.rules = json.load(f)
        else:
            self.rules = []

    def evaluate_text(self, text: str):
        """í…ìŠ¤íŠ¸ ë‚´ ì—¬ëŸ¬ ê·œì¹™ ë§¤ì¹­ (yesë§Œ ê°ì§€)"""
        detected = []
        for rule in self.rules:
            pattern = rule.get("pattern", "")
            verdict = rule.get("verdict", "yes")
            if re.search(rf".*{pattern}.*", text, re.IGNORECASE):
                if verdict == "yes":
                    detected.append(pattern)
        return detected

    def evaluate_dialogue(self, user_input: str, ai_reply: str):
        """ì‚¬ìš©ì ì…ë ¥ì—ì„œ ë‹¤ì¤‘ ë‹¨ì„œ ê°ì§€"""
        user_clues = self.evaluate_text(user_input)
        if user_clues:
            return {
                "text": "ğŸ’¡ í¥ë¯¸ë¡œìš´ ë‹¨ì„œê°€ ì–¸ê¸‰ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤.",
                "clues": user_clues
            }
        return None

    # ë‹¨ì„œ ì¶”ì¶œ ë©”ì„œë“œ
    def extract_clue_from_feedback(self, feedback_text: str):
        """í”¼ë“œë°± ë¬¸ì¥ì—ì„œ ë‹¨ì„œëª…ë§Œ ì¶”ì¶œ"""
        match = re.search(r"['\"](.+?)['\"]", feedback_text)
        if match:
            return match.group(1)
        return None

app = FastAPI()
# ------------------------------
# FastAPI ê¸°ë³¸ ì„¤ì •
# ------------------------------
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ì„ì‹œë¡œ ëª¨ë“  origin í—ˆìš© (í…ŒìŠ¤íŠ¸ìš©)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

BASE_DIR = Path(__file__).resolve().parent

# =============================
# ì‚¬ê±´ ë°ì´í„° ë¡œë”
# =============================
def load_case_data(mode: str):
    base_dir = os.path.join(os.path.dirname(__file__), "cases")
    filename = {
        "ìƒ": "case_high.json",
        "ì¤‘": "case_mid.json",
        "í•˜": "case_low.json",
    }.get(mode, "case_low.json")
    path = os.path.join(base_dir, filename)
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


# =============================
# GPT ì‘ë‹µ ìƒì„± (íˆìŠ¤í† ë¦¬ ë°˜ì˜)
# =============================
def generate_gpt_response(case_data, user_input, history):
    """
    ì‚¬ê±´ ë°ì´í„°ì™€ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ GPT ì‘ë‹µì„ ìƒì„±.
    - ì‚¬ê±´ JSON(case_data) ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•˜ë„ë¡ ì œí•œ
    - ìì—°ìŠ¤ëŸ¬ìš´ íƒì • ì¡°ìˆ˜ ìŠ¤íƒ€ì¼ ìœ ì§€
    - ë„ˆë¬´ ê¸´ ì‘ë‹µ ë°©ì§€ (150ì ë‚´ì™¸)
    """

    chatbot_instr = case_data.get("chatbot_instructions", {})
    role = chatbot_instr.get("role", "ë„ˆëŠ” ì‚¬ê±´ì„ í•¨ê»˜ ì¶”ë¦¬í•˜ëŠ” íƒì • ì¡°ìˆ˜ AIì•¼.")
    style = chatbot_instr.get("style", "ëƒ‰ì •í•˜ê³  ì¹¨ì°©í•œ ë§íˆ¬ë¡œ, íƒì •ì²˜ëŸ¼ ê°„ê²°í•˜ê²Œ ë§í•´.")
    guidelines_list = chatbot_instr.get("guidelines", [])

    # ì§€ì¹¨ì„ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬
    guidelines = "\n".join(f"- {g}" for g in guidelines_list) if guidelines_list else ""

    # ì‚¬ê±´ ìš”ì•½ ë¸”ë¡ ì¶”ì¶œ (GPTê°€ ë¹ ë¥´ê²Œ ì°¸ì¡°)
    summary_info = case_data.get("summary_info", {})
    summary_text = json.dumps(summary_info, ensure_ascii=False)

    # ì£¼ìš” ì„¹ì…˜ í…ìŠ¤íŠ¸ ë³‘í•© (GPTê°€ ë¬¸ë§¥ ê¸°ë°˜ ì¶”ë¡  ê°€ëŠ¥)
    overview_text = json.dumps(case_data.get("case_overview", {}), ensure_ascii=False)
    character_text = json.dumps(case_data.get("characters", []), ensure_ascii=False)
    evidence_text = json.dumps(case_data.get("evidence", []), ensure_ascii=False)
    case_flow_text = json.dumps(case_data.get("case_flow", {}), ensure_ascii=False)

    # SYSTEM PROMPT
    system_prompt = f"""
{role}
{style}

ë„ˆëŠ” ì•„ë˜ ì‚¬ê±´ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•´ì•¼ í•´.
ì ˆëŒ€ ìŠ¤ìŠ¤ë¡œ ì¶”ë¦¬í•˜ê±°ë‚˜ ìƒìƒí•˜ì§€ ë§ê³ , ì£¼ì–´ì§„ ì‚¬ì‹¤ì— ê·¼ê±°í•´ì„œë§Œ ë§í•´.
ë§Œì•½ ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì´ë©´, "ê·¸ ì •ë³´ëŠ” ì•„ì§ ì¡°ì‚¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•´.

[ë‹µë³€ ê·œì¹™]
- ë‹µë³€ì€ 150ì ì´ë‚´ë¡œ ì œí•œ.
- í•µì‹¬ë§Œ ìš”ì•½í•´ 1~3ë¬¸ì¥ìœ¼ë¡œ ë§í•´.
- ë¶ˆí•„ìš”í•œ ì„œìˆ ì´ë‚˜ ì¥ë¬¸ ë¶„ì„ì€ ê¸ˆì§€.
- ë‹¨ì„œë‚˜ ì¦ê±°ëŠ” ìš”ì•½ëœ í˜•íƒœë¡œë§Œ ì–¸ê¸‰.

[ì‚¬ê±´ ìš”ì•½ ì •ë³´]
{summary_text}

[ì‚¬ê±´ ê°œìš”]
{overview_text}

[ë“±ì¥ì¸ë¬¼]
{character_text}

[ì¦ê±° ëª©ë¡]
{evidence_text}

[ì‚¬ê±´ íë¦„ ìš”ì•½]
{case_flow_text}

[ëŒ€í™” ì§€ì¹¨]
{guidelines}
    """.strip()

    messages = [{"role": "system", "content": system_prompt}]
    for h in history:
        messages.append({"role": h["role"], "content": h["text"]})
    messages.append({"role": "user", "content": user_input})
    
    # GPT í˜¸ì¶œ
    print("[GPT í˜¸ì¶œ ì‹œì‘]")
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        # model="gpt-5"
        messages=messages,
        temperature=0.6,  # ë‹µë³€ì˜ ì¼ê´€ì„±ê³¼ ë…¼ë¦¬ì„± ìœ ì§€
        max_tokens=200
    )
    print("[GPT ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ]")

    # ìµœì¢… ì‘ë‹µ ë°˜í™˜
    reply = completion.choices[0].message.content.strip()
    return reply


# =============================
# API: ëŒ€í™” ì—”ë“œí¬ì¸íŠ¸
# =============================
@app.post("/api/chat")
async def chat_endpoint(request: Request):
    start_time = time.time()
    print("\n[CHAT] ìš”ì²­ ì‹œì‘ ------------------------")
    
    try:
        data = await request.json()
        user_input = data.get("message", "")
        mode = data.get("mode", "í•˜")
        history = data.get("history", [])
        print(f"ì…ë ¥: {user_input[:80]}... (ëª¨ë“œ={mode})")

        # ì‚¬ê±´ ë°ì´í„° ë¡œë“œ
        case_data = load_case_data(mode)
        case_id = f"case_{'high' if mode == 'ìƒ' else 'mid' if mode == 'ì¤‘' else 'low'}"
        logic_engine = DesertLogicEngine(case_id)
        print(f"ì‚¬ê±´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({case_id})")

        # ì¸ë¬¼ ì´ë¦„ ì¦‰ì‹œ ê°ì§€
        for c in case_data.get("characters", []):
            # ì´ë¦„ì´ ì…ë ¥ë¬¸ì— í¬í•¨ë˜ë©´ ì¦‰ì‹œ ë°˜í™˜
            if c["name"] in user_input:
                print(f"ì¸ë¬¼ ê°ì§€: {c['name']}")
                return {
                    "reply": f"{c['name']} â€” {c['description']}",
                    "clue": c["name"],  # í”„ë¡ íŠ¸ì— ì „ë‹¬ (ê°ì§€ ë‹¨ì„œë¡œ)
                }

        # ì¦ê±° ì´ë¦„(type) ì¦‰ì‹œ ê°ì§€(ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª… í¬í•¨)
        for e in case_data.get("evidence", []):
            evidence_type = e.get("type")
            desc = e.get("description", "")
            extra = e.get("spoiler_investigation", "")

            if evidence_type and evidence_type in user_input:
                print(f"ì¦ê±° ê°ì§€: {evidence_type}")

                # descriptionê³¼ ì¶”ê°€ ì¡°ì‚¬ ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í•©ì¹¨
                reply_parts = []
                if desc:
                    reply_parts.append(desc.strip().rstrip("."))
                if extra:
                    reply_parts.append(f"ì¶”ê°€ ì¡°ì‚¬ ê²°ê³¼, {extra.strip().rstrip('.')}")
                
                # ì—¬ë°± ìˆëŠ” ì¶œë ¥: ê° ë¬¸ì¥ë§ˆë‹¤ ì¤„ë°”ê¿ˆ
                formatted_text = "\n\n".join(reply_parts)

                # íƒì •ì‹ í†¤ ì¶”ê°€
                reply_text = f"{evidence_type}ì€(ëŠ”) {formatted_text}."

                return {
                    "reply": reply_text.strip(),
                    "clue": evidence_type,
                }

        # GPT ì‘ë‹µ ìƒì„± (íˆìŠ¤í† ë¦¬ ë°˜ì˜)
        t1 = time.time()
        ai_reply = generate_gpt_response(case_data, user_input, history)
        print(f"GPT ì‘ë‹µ ìƒì„± ì™„ë£Œ ({time.time() - t1:.2f}s)")

        # Desert Logic ê²€ì¦(Desert Logicì´ ê°ì§€í•œ ë‹¨ì„œ/ë…¼ë¦¬ í‰ê°€ ê²°ê³¼)
        t2 = time.time()
        try:
            logic_feedback = logic_engine.evaluate_dialogue(user_input, ai_reply)
        except Exception as e:
            print(f"[LogicEngine ì˜¤ë¥˜] {e}")
            traceback.print_exc()
        print(f"ğŸ” ë‹¨ì„œ ê°ì§€ ì™„ë£Œ ({time.time() - t2:.2f}s)")

        # ê°ì§€ëœ ë‹¨ì„œ ì¶”ì¶œ
        clue_data = None
        if logic_feedback:
            clue_data = logic_feedback.get("clues", [])

        # ìµœì¢… ì‘ë‹µ êµ¬ì„±
        if logic_feedback:
            final_reply = f"{ai_reply}\n\n---\n{logic_feedback['text']}"
        else:
            final_reply = ai_reply
            
        print(f"ğŸ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ ({time.time() - start_time:.2f}s)")
        print("-------------------------------------------\n")
            
        return {
            "reply": final_reply,  # í”„ë¡ íŠ¸ì—”ë“œë¡œ ì „ì†¡ë˜ëŠ” JSON ì‘ë‹µ
            "clues": clue_data  # ë°°ì—´ í˜•íƒœë¡œ ì—¬ëŸ¬ ë‹¨ì„œ ì „ë‹¬
            }
        
    except Exception as e:
        print(f"ğŸ’¥ [chat_endpoint ì˜ˆì™¸ ë°œìƒ]: {e}")
        traceback.print_exc()
        return {"error": str(e)}


# =============================
# ì‚¬ê±´ ì •ë³´ ë°˜í™˜, ìŠ¤í† ë¦¬ txt ë°˜í™˜
# =============================
@app.get("/api/report")
async def report_endpoint(mode: str = "í•˜"):
    case_data = load_case_data(mode)
    
    # ìŠ¤í† ë¦¬ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    story_path = os.path.join(os.path.dirname(__file__), "cases", "story")
    story_file = {
        "ìƒ": "story_high.txt",
        "ì¤‘": "story_mid.txt",
        "í•˜": "story_low.txt",
    }.get(mode, "story_low.txt")
    story_fullpath = os.path.join(story_path, story_file)

    # ìŠ¤í† ë¦¬ í…ìŠ¤íŠ¸ ë¡œë“œ
    story_text = ""
    if os.path.exists(story_fullpath):
        with open(story_fullpath, "r", encoding="utf-8") as f:
            story_text = f.read()
    else:
        story_text = "ìŠ¤í† ë¦¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ì‚¬ê±´ ë°ì´í„° + ìŠ¤í† ë¦¬ í•¨ê»˜ ë°˜í™˜
    return {"case": case_data, "story": story_text}

# =============================
# AI ì±„ì  API
# =============================
@app.post("/api/submit_answer")
async def submit_answer(request: Request):
    data = await request.json()
    mode = data.get("mode", "í•˜")
    user_answer = data.get("answer", "")

    case_data = load_case_data(mode)
    solution = case_data.get("solution", {})
    reference = json.dumps(solution, ensure_ascii=False)

    prompt = f"""
    ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì¶”ë¦¬ ë‹µë³€ì…ë‹ˆë‹¤.
    ì‚¬ê±´ ì œëª©: {case_data.get('title')}
    ì •ë‹µ ë°ì´í„°: {reference}
    ì‚¬ìš©ì ë‹µë³€: {user_answer}
    
    âš ï¸ ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì—ëŠ” 0ì ì„ ì£¼ê³  ê°„ë‹¨íˆ í”¼ë“œë°±í•˜ì„¸ìš”:
    - ì‚¬ìš©ìê°€ ë‹¨ìˆœíˆ "ì¶”ë¦¬ ì‘ì„±", "ì‚¬ê±´ ê°œìš”", "ì—”ë”© ë³´ê¸°" ë“± í˜•ì‹ì  ë¬¸êµ¬ë§Œ ì…ë ¥í•œ ê²½ìš°
    - ì¶”ë¦¬ì˜ ë‚´ìš©ì´ ì „í˜€ ì—†ëŠ” ê²½ìš° (ë²”ì¸, ë™ê¸°, ì‚¬ê±´ ë‚´ìš© ì—†ìŒ)
    - ë‹¨ìˆœí•œ ëª…ë ¹ë¬¸, í…ŒìŠ¤íŠ¸ ë¬¸ì¥, í•œ ì¤„ì§œë¦¬ ì…ë ¥

    ê¸°ì¤€:
    1. ë²”ì¸, ë™ê¸°, ë°©ë²•, ê²°ë¡  ì¼ì¹˜ë„ (ì´ì  100ì )
    2. ë…¼ë¦¬ ì¼ê´€ì„± ë° ë‹¨ì„œ í™œìš©ë„ (+/- 20ì  ê°€ì¤‘)
    3. í•µì‹¬ ì§„ì‹¤ ëˆ„ë½ ì‹œ ê°ì 
    4. í”¼ë“œë°±ì€ ê°„ê²°í•˜ê³  ì¸ë¬¼Â·ì¦ê±° ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±

    JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
    {{
      "score": <0~100>,
      "feedback": "<ì§§ì€ í‰ê°€ ì½”ë©˜íŠ¸>"
    }}
    """

    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        # model="gpt-5",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ê³µí¬ ì¶”ë¦¬ ê²Œì„ì˜ ì±„ì ê´€ AIì•¼. ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ë˜, ì•½ê°„ ë¶ˆì•ˆí•œ ë§íˆ¬ë¥¼ ì¨."},
            {"role": "user", "content": prompt}
        ]
    )

    try:
        result = json.loads(completion.choices[0].message.content)
    except:
        result = {"score": 0, "feedback": "ì±„ì  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

    return result