import json, os, re
from pathlib import Path
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
from openai import OpenAI
from fastapi.responses import PlainTextResponse
from logic_engine.desert_logic import DesertLogicEngine 

import time, traceback


# ------------------------------
# â‘  .env ë¶ˆëŸ¬ì˜¤ê¸°
# ------------------------------
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

with open("logic_engine/rules/case_mid_rules.json", "r", encoding="utf-8") as f:
    rules = json.load(f)

for r in rules:
    pattern = r.get("pattern", "")
    if not pattern.strip():
        print("âš ï¸ ë¹ˆ íŒ¨í„´ ê°ì§€:", r)

app = FastAPI()
# ------------------------------
# FastAPI ê¸°ë³¸ ì„¤ì •
# ------------------------------
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ì„ì‹œë¡œ ëª¨ë“  origin í—ˆìš© (í…ŒìŠ¤íŠ¸ìš©)
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

BASE_DIR = Path(__file__).resolve().parent

# =============================
# ì‚¬ê±´ ë°ì´í„° ë¡œë”
# =============================
def load_case_data(mode: str):
    base_dir = os.path.join(os.path.dirname(__file__), "cases")
    filename = {
        "ìƒ": "case_high.json",
        "ì¤‘": "case_mid.json",
        "í•˜": "case_low.json",
    }.get(mode, "case_low.json")
    path = os.path.join(base_dir, filename)
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


# =============================
# GPT ì‘ë‹µ ìƒì„± (íˆìŠ¤í† ë¦¬ ë°˜ì˜)
# =============================
def generate_gpt_response(case_data, user_input, history):
    """
    ì‚¬ê±´ ë°ì´í„°ì™€ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ GPT ì‘ë‹µì„ ìƒì„±.
    - ì‚¬ê±´ JSON(case_data) ê¸°ë°˜ìœ¼ë¡œë§Œ ë‹µë³€í•˜ë„ë¡ ì œí•œ
    - ìì—°ìŠ¤ëŸ¬ìš´ íƒì • ì¡°ìˆ˜ ìŠ¤íƒ€ì¼ ìœ ì§€
    - ë„ˆë¬´ ê¸´ ì‘ë‹µ ë°©ì§€ (200ì ë‚´ì™¸)
    """

    chatbot_instr = case_data.get("chatbot_instructions", {})
    role = chatbot_instr.get("role", "ë„ˆëŠ” ì‚¬ê±´ì„ í•¨ê»˜ ì¶”ë¦¬í•˜ëŠ” íƒì • ì¡°ìˆ˜ AIì•¼.")
    style = chatbot_instr.get("style", "ëƒ‰ì •í•˜ê³  ì¹¨ì°©í•œ ë§íˆ¬ë¡œ, íƒì •ì²˜ëŸ¼ ê°„ê²°í•˜ê²Œ ë§í•´.")
    guidelines_list = chatbot_instr.get("guidelines", [])

    # ì§€ì¹¨ì„ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬
    guidelines = "\n".join(f"- {g}" for g in guidelines_list) if guidelines_list else ""

    # ì‚¬ê±´ ìš”ì•½ ë¸”ë¡ ì¶”ì¶œ (GPTê°€ ë¹ ë¥´ê²Œ ì°¸ì¡°)
    summary_info = case_data.get("summary_info", {})
    summary_text = json.dumps(summary_info, ensure_ascii=False)

    # ì£¼ìš” ì„¹ì…˜ í…ìŠ¤íŠ¸ ë³‘í•© (GPTê°€ ë¬¸ë§¥ ê¸°ë°˜ ì¶”ë¡  ê°€ëŠ¥)
    overview_text = json.dumps(case_data.get("case_overview", {}), ensure_ascii=False)
    character_text = json.dumps(case_data.get("characters", []), ensure_ascii=False)
    evidence_text = json.dumps(case_data.get("evidence", []), ensure_ascii=False)
    case_flow_text = json.dumps(case_data.get("case_flow", {}), ensure_ascii=False)

    # SYSTEM PROMPT
    system_prompt = f"""
{role}
{style}

ë„ˆëŠ” ì•„ë˜ ì‚¬ê±´ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•´ì•¼ í•´.
ì ˆëŒ€ ìŠ¤ìŠ¤ë¡œ ì¶”ë¦¬í•˜ê±°ë‚˜ ìƒìƒí•˜ì§€ ë§ê³ , ì£¼ì–´ì§„ ì‚¬ì‹¤ì— ê·¼ê±°í•´ì„œë§Œ ë§í•´.
ë§Œì•½ ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì´ë©´, "ê·¸ ì •ë³´ëŠ” ì•„ì§ ì¡°ì‚¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•´.

[ë‹µë³€ ê·œì¹™]
- ë‹µë³€ì€ 200ì ì´ë‚´ë¡œ ì œí•œ.
- í•µì‹¬ë§Œ ìš”ì•½í•´ 1~3ë¬¸ì¥ìœ¼ë¡œ ë§í•´.
- ë¶ˆí•„ìš”í•œ ì„œìˆ ì´ë‚˜ ì¥ë¬¸ ë¶„ì„ì€ ê¸ˆì§€.
- ë‹¨ì„œë‚˜ ì¦ê±°ëŠ” ìš”ì•½ëœ í˜•íƒœë¡œë§Œ ì–¸ê¸‰.

[ì‚¬ê±´ ìš”ì•½ ì •ë³´]
{summary_text}

[ì‚¬ê±´ ê°œìš”]
{overview_text}

[ë“±ì¥ì¸ë¬¼]
{character_text}

[ì¦ê±° ëª©ë¡]
{evidence_text}

[ì‚¬ê±´ íë¦„ ìš”ì•½]
{case_flow_text}

[ëŒ€í™” ì§€ì¹¨]
{guidelines}
    """.strip()

    messages = [{"role": "system", "content": system_prompt}]
    for h in history:
        messages.append({"role": h["role"], "content": h["text"]})
    messages.append({"role": "user", "content": user_input})
    
    # GPT í˜¸ì¶œ
    print("[GPT í˜¸ì¶œ ì‹œì‘]")
    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        # model="gpt-5"
        messages=messages,
        temperature=0.6,  # ë‹µë³€ì˜ ì¼ê´€ì„±ê³¼ ë…¼ë¦¬ì„± ìœ ì§€
        max_tokens=200
    )
    print("[GPT ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ]")

    # ìµœì¢… ì‘ë‹µ ë°˜í™˜
    reply = completion.choices[0].message.content.strip()
    return reply


# =============================
# API: ëŒ€í™” ì—”ë“œí¬ì¸íŠ¸
# =============================
@app.post("/api/chat")
async def chat_endpoint(request: Request):
    start_time = time.time()
    print("\n[CHAT] ìš”ì²­ ì‹œì‘ ------------------------")

    try:
        data = await request.json()
        user_input = data.get("message", "")
        mode = data.get("mode", "í•˜")
        history = data.get("history", [])
        print(f"ì…ë ¥: {user_input[:80]}... (ëª¨ë“œ={mode})")

        # ì‚¬ê±´ ë°ì´í„° ë¡œë“œ
        case_data = load_case_data(mode)
        case_id = f"case_{'high' if mode == 'ìƒ' else 'mid' if mode == 'ì¤‘' else 'low'}"
        logic_engine = DesertLogicEngine(case_id)
        print(f"ì‚¬ê±´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({case_id})")

        # GPT ì‘ë‹µ ìƒì„±
        ai_reply = generate_gpt_response(case_data, user_input, history)
        combined_text = f"{user_input}\n{ai_reply}"

        # ------------------------------
        # ê°ì§€ ê²°ê³¼ ëˆ„ì ìš©
        # ------------------------------
        detected_evidences = set()
        detected_characters = set()
        new_clues = set()
        extra_info = []
        all_hints = []

        # --- 1) ì¸ë¬¼ ê°ì§€ ---
        for c in case_data.get("characters", []):
            name = c["name"]
            if name in combined_text:
                detected_characters.add(name)
                new_clues.add(name)

                # ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ ë“±ì¥í•œ ê²½ìš°ë§Œ ì¶”ê°€ ì„¤ëª…
                if name in user_input:
                    extra_info.append(f"ğŸ’¡ {name} â€” {c['description']}")
                    
        # --- 2) ì¦ê±° ê°ì§€ ---
        for e in case_data.get("evidence", []):
            evidence_type = e.get("type")
            desc = e.get("description", "")
            extra = e.get("spoiler_investigation", "")

            if not evidence_type:
                continue

            # ì˜¤ì§ ì‚¬ìš©ì ì…ë ¥(user_input)ì— ë‹¨ì„œê°€ í¬í•¨ëœ ê²½ìš°ë§Œ ê°ì§€
            if re.search(evidence_type, user_input, re.IGNORECASE):
                # ì´ë¯¸ ê°ì§€ëœ ë‹¨ì„œëŠ” ê±´ë„ˆë›°ê¸°
                if evidence_type in detected_evidences:
                    continue

                detected_evidences.add(evidence_type)
                new_clues.add(evidence_type)

                # ğŸ’­ ì¡°ìˆ˜ì˜ ìƒê°ìš© hint ì¶”ì¶œ (ì¤‘ë³µ ë°©ì§€)
                hint_text = logic_engine.get_hint_for_evidence(evidence_type)
                if hint_text and evidence_type not in [h.split(":")[0] for h in all_hints]:
                    all_hints.append(f"{evidence_type}: {hint_text}")

                # ğŸ’¡ ê¸°ì¡´ ìš”ì•½ ë¬¸êµ¬ ì¶”ê°€
                if desc:
                    reply_text = f"ğŸ’¡ {evidence_type}ì€(ëŠ”) {desc.strip().rstrip('.')}."
                    extra_info.append(reply_text)


        # --- 3) Desert Logic ---
        try:
            logic_feedback = logic_engine.evaluate_dialogue(combined_text, ai_reply)
        except Exception as e:
            print(f"[LogicEngine ì˜¤ë¥˜] {e}")
            traceback.print_exc()
            logic_feedback = None

        # ------------------------------
        # ìµœì¢… ì‘ë‹µ êµ¬ì„±
        # ------------------------------
        final_reply = ai_reply

        # (1) ì¡°ì‚¬ ìš”ì²­ ì‹œ ì¶”ê°€ ì •ë³´ í‘œì‹œ
        if extra_info:
            final_reply += "\n\n" + "\n".join(extra_info)

        # (2) ì¶”ê°€ ì„¤ëª…ì´ ì—†ê³  ìƒˆ ë‹¨ì„œê°€ ê°ì§€ëœ ê²½ìš°ì—ë§Œ â†’ ìš”ì•½ ë©”ì‹œì§€ í•œ ì¤„ í‘œì‹œ
        previous_clues = [h.get("text", "") for h in history if isinstance(h, dict)]  # ì´ì „ ëŒ€í™” í…ìŠ¤íŠ¸ë“¤
        previous_text = " ".join(previous_clues)

        # ìƒˆë¡œ ë°œê²¬ëœ ë‹¨ì„œë§Œ í•„í„°ë§
        unique_new_clues = [
            clue for clue in (new_clues or [])
            if clue not in previous_text
        ]

        if (
            not extra_info
            and unique_new_clues  # ì´ì „ì— ì—†ë˜ ë‹¨ì„œë§Œ ìˆì„ ë•Œ
            and not any(keyword in user_input for keyword in ["ì¡°ì‚¬", "ì‚´í´", "í™•ì¸", "ìì„¸íˆ"])
        ):
            final_reply += "\n\nğŸ’¡ í¥ë¯¸ë¡œìš´ ë‹¨ì„œê°€ ì–¸ê¸‰ëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤."


        # (3) Desert Logic ë¬¸ì¥ì€ í•œ ë²ˆë§Œ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
        if logic_feedback and logic_feedback.get("text"):
            # textê°€ Noneì´ ì•„ë‹ ë•Œë§Œ ì¶”ê°€
            text_msg = logic_feedback.get("text")
            if text_msg and "í¥ë¯¸ë¡œìš´ ë‹¨ì„œ" not in text_msg:
                final_reply += f"\n\n---\n{text_msg}"

        # (4) ê°ì§€ëœ ë‹¨ì„œ í†µí•© (ì¤‘ë³µ ì œê±°)
        all_clues = list(set(list(new_clues) + (logic_feedback.get("clues", []) if logic_feedback else [])))
        
        print(f"ğŸ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ ({time.time() - start_time:.2f}s)")
        print("-------------------------------------------\n")
        
        print(f"[DEBUG] all_hints ìµœì¢…: {all_hints}")
        return {
            "reply": final_reply,
            "clues": list(set(list(new_clues))),
            "hints": all_hints  # ì¡°ìˆ˜ì˜ ìƒê° ë”°ë¡œ ë‚´ë ¤ì¤Œ
        }

    except Exception as e:
        print(f"ğŸ’¥ [chat_endpoint ì˜ˆì™¸ ë°œìƒ]: {e}")
        traceback.print_exc()
        return {"error": str(e)} 


# =============================
# ì‚¬ê±´ ì •ë³´ ë°˜í™˜, ìŠ¤í† ë¦¬ txt ë°˜í™˜
# =============================
@app.get("/api/report")
async def report_endpoint(mode: str = "í•˜"):
    case_data = load_case_data(mode)
    
    # ìŠ¤í† ë¦¬ íŒŒì¼ ê²½ë¡œ ì„¤ì •
    story_path = os.path.join(os.path.dirname(__file__), "cases", "story_summary")
    story_file = {
        "ìƒ": "story_high.txt",
        "ì¤‘": "story_mid.txt",
        "í•˜": "story_low.txt",
    }.get(mode, "story_low.txt")
    story_fullpath = os.path.join(story_path, story_file)

    # ìŠ¤í† ë¦¬ í…ìŠ¤íŠ¸ ë¡œë“œ
    story_text = ""
    if os.path.exists(story_fullpath):
        with open(story_fullpath, "r", encoding="utf-8") as f:
            story_text = f.read()
    else:
        story_text = "ìŠ¤í† ë¦¬ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ì‚¬ê±´ ë°ì´í„° + ìŠ¤í† ë¦¬ í•¨ê»˜ ë°˜í™˜
    return {"case": case_data, "story": story_text}

# =============================
# AI ì±„ì  API
# =============================
@app.post("/api/submit_answer")
async def submit_answer(request: Request):
    data = await request.json()
    mode = data.get("mode", "í•˜")
    user_answer = data.get("answer", "")

    case_data = load_case_data(mode)
    solution = case_data.get("solution", {})
    reference = json.dumps(solution, ensure_ascii=False)

    prompt = f"""
    ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ì¶”ë¦¬ ë‹µë³€ì…ë‹ˆë‹¤.
    ì‚¬ê±´ ì œëª©: {case_data.get('title')}
    ì •ë‹µ ë°ì´í„°: {reference}
    ì‚¬ìš©ì ë‹µë³€: {user_answer}
    
    âš ï¸ ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì—ëŠ” 0ì ì„ ì£¼ê³  ê°„ë‹¨íˆ í”¼ë“œë°±í•˜ì„¸ìš”:
    - ì‚¬ìš©ìê°€ ë‹¨ìˆœíˆ "ì¶”ë¦¬ ì‘ì„±", "ì‚¬ê±´ ê°œìš”", "ì—”ë”© ë³´ê¸°" ë“± í˜•ì‹ì  ë¬¸êµ¬ë§Œ ì…ë ¥í•œ ê²½ìš°
    - ì¶”ë¦¬ì˜ ë‚´ìš©ì´ ì „í˜€ ì—†ëŠ” ê²½ìš° (ë²”ì¸, ë™ê¸°, ì‚¬ê±´ ë‚´ìš© ì—†ìŒ)
    - ë‹¨ìˆœí•œ ëª…ë ¹ë¬¸, í…ŒìŠ¤íŠ¸ ë¬¸ì¥, í•œ ì¤„ì§œë¦¬ ì…ë ¥

    ê¸°ì¤€:
    1. ë²”ì¸, ë™ê¸°, ë°©ë²•, ê²°ë¡  ì¼ì¹˜ë„ (ì´ì  100ì )
    2. ë…¼ë¦¬ ì¼ê´€ì„± ë° ë‹¨ì„œ í™œìš©ë„ (+/- 20ì  ê°€ì¤‘)
    3. í•µì‹¬ ì§„ì‹¤ ëˆ„ë½ ì‹œ ê°ì 
    4. í”¼ë“œë°±ì€ ê°„ê²°í•˜ê³  ì¸ë¬¼Â·ì¦ê±° ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±

    JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
    {{
      "score": <0~100>,
      "feedback": "<ì§§ì€ í‰ê°€ ì½”ë©˜íŠ¸>"
    }}
    """

    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        # model="gpt-5",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ê³µí¬ ì¶”ë¦¬ ê²Œì„ì˜ ì±„ì ê´€ AIì•¼. ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ë˜, ì•½ê°„ ë¶ˆì•ˆí•œ ë§íˆ¬ë¥¼ ì¨."},
            {"role": "user", "content": prompt}
        ]
    )

    try:
        result = json.loads(completion.choices[0].message.content)
    except:
        result = {"score": 0, "feedback": "ì±„ì  ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}

    return result